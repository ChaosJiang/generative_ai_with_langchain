{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0054fc",
   "metadata": {},
   "source": [
    "#Basic examples\n",
    "\n",
    "###Make sure you load the API keys for cloud providers!\n",
    "\n",
    "###You can set your environment keys yourself or use a script. Please note that since keys are private, they are not included in the repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f78f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from config import set_environment\n",
    "\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d43c3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the light bulb get an award?\n",
      "Because it was outstanding in its field! (Or, because it had so many bright ideas!)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "openai_llm = OpenAI()\n",
    "google_llm = GoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\")\n",
    "\n",
    "# response = openai_llm.invoke(\"Tell me a joke about light bulbs!\")\n",
    "response = google_llm.invoke(\"Tell me a joke about light bulbs!\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d84b08",
   "metadata": {},
   "source": [
    "###Development testing, FakeListLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf97447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import FakeListLLM\n",
    "\n",
    "fake_llm = FakeListLLM(responses=[\"Hello\"])\n",
    "result = fake_llm.invoke(\"Tell me a joke about light bulbs!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c357967",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e077f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Certainly! You can calculate the factorial of a number using a simple recursive function or an iterative approach. Here\\'s how you can implement both methods in Python:\\n\\n1. **Recursive approach**:\\n\\n```python\\ndef factorial_recursive(n):\\n    if n < 0:\\n        raise ValueError(\"Factorial is not defined for negative numbers\")\\n    if n == 0 or n == 1:\\n        return 1\\n    else:\\n        return n * factorial_recursive(n - 1)\\n\\n# Example usage:\\nprint(factorial_recursive(5))  # Output: 120\\n```\\n\\n2. **Iterative approach**:\\n\\n```python\\ndef factorial_iterative(n):\\n    if n < 0:\\n        raise ValueError(\"Factorial is not defined for negative numbers\")\\n    result = 1\\n    for i in range(2, n + 1):\\n        result *= i\\n    return result\\n\\n# Example usage:\\nprint(factorial_iterative(5))  # Output: 120\\n```\\n\\nBoth functions will calculate the factorial of a given non-negative integer `n`. Please note that factorial is only defined for non-negative integers, so it\\'s good practice to handle invalid input by raising an exception, as shown in the examples above.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 253, 'prompt_tokens': 24, 'total_tokens': 277, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9bddfca6e2', 'id': 'chatcmpl-Bg8vsiv3fA8f2kEqp4dJzxwuAMCVN', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--0b8bbc66-62bd-4cfa-bd8c-ab8f0b338252-0' usage_metadata={'input_tokens': 24, 'output_tokens': 253, 'total_tokens': 277, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# chat = ChatAnthropic(model_name=\"claude-3-5-sonnet-latest\", timeout=None, stop=None)\n",
    "chat = ChatOpenAI(model=\"gpt-4o\")\n",
    "messages = [\n",
    "    SystemMessage(\"You're a helpful programming assistant\"),\n",
    "    HumanMessage(\"Write a Python function to calculate factorial.\"),\n",
    "]\n",
    "response = chat.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ceea1",
   "metadata": {},
   "source": [
    "### Openai with thinking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2555fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a problem-solving assistant.\"),\n",
    "        (\"user\", \"{ problem}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model=\"o3-mini\", reasoning_effort=\"high\")\n",
    "chain = template | chat\n",
    "response = chain.invoke({\"problem\": \"Calculate the optimal strategy for...\"})\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o\")\n",
    "chain = template | chat\n",
    "response = chain.invoke({\"problem\": \"Calculate the optimal strategy for...\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e3425",
   "metadata": {},
   "source": [
    "### Simple workflows with LCEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4370aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do programmers prefer dark mode? \n",
      "\n",
      "Because light attracts bugs!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create components\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "llm = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Chain them together using LCEL\n",
    "chain = prompt | llm | output_parser\n",
    "# Execute the workflow with a single call\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc30daf",
   "metadata": {},
   "source": [
    "### Complex chain example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "613c58a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mood of this story is overwhelmingly **peaceful, serene, and introspective**, with strong undertones of **comfort, contentment, and quiet restoration.**\n",
      "\n",
      "Here's a breakdown of how this mood is established:\n",
      "\n",
      "1.  **Sensory Details Emphasizing Gentleness and Softness:**\n",
      "    *   **Visuals:** \"watercolor blur,\" \"shades of grey and muted green,\" \"tiny rivers formed on the glass, merging and separating,\" \"light outside softened even further, a deep twilight.\" These images are soft, calming, and inviting to contemplation rather than action.\n",
      "    *   **Sounds:** The rain is described as a \"steady drumming,\" a \"rhythmic hiss,\" a \"persistent, gentle presence,\" and \"nature's soft lullaby.\" The \"relentless hum of the city\" fades, replaced by this soothing natural soundscape. The kettle \"whistled a soft tune.\" The rain is a \"steady, comforting murmur,\" and the world \"whisper[s].\" The absence of harsh or demanding sounds is key.\n",
      "    *   **Smell:** \"faint, clean scent of damp earth and old books\" evokes a sense of coziness and natural freshness.\n",
      "    *   **Touch/Temperature:** \"Air inside the apartment felt cooler,\" the \"warmth seeping into her hands\" from the tea. These are gentle, comforting sensations.\n",
      "\n",
      "2.  **Character's Internal State and Actions:**\n",
      "    *   Elara is \"curled in her favorite armchair,\" her book \"forgotten.\" This posture and action immediately signal relaxation and a lack of urgency.\n",
      "    *   The rain \"encouraged introspection, drawing the world inward, hushing the usual clamor.\" Her breathing becomes \"deeper,\" and \"tension in her shoulders easing.\" These are direct indicators of a mind and body letting go of stress.\n",
      "    *   She doesn't pick up the book; instead, she \"just sat, sipping her tea, listening.\" This reflects a complete surrender to the moment and a deep immersion in the quiet atmosphere.\n",
      "    *   Her \"sigh, not of sadness, but of profound contentment,\" and her feeling of \"a deep sense of peace\" explicitly state the prevailing emotional state.\n",
      "\n",
      "3.  **Themes of Release and Cleansing:**\n",
      "    *   The world \"didn't demand anything, didn't beckon with urgent tasks or social obligations.\" This highlights a liberation from external pressures.\n",
      "    *   The rain is personified as \"washing away the week's anxieties, the fleeting worries, the minor irritations.\" It's a \"cleansing, a reset.\" This suggests a transformative and restorative power to the quiet, rainy day.\n",
      "    *   The concluding thought, \"the most profound experiences came not from grand adventures, but from simply being present, listening to the world whisper,\" encapsulates the story's celebration of quiet, internal richness.\n",
      "\n",
      "In essence, the mood is one of profound **tranquility and gentle escape**, inviting the reader to share in Elara's peaceful withdrawal from the outside world and her rediscovery of inner calm through the simple, comforting presence of the rain.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the model\n",
    "llm = GoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\")\n",
    "\n",
    "# First chain generates s story\n",
    "story_prompt = PromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "story_chain = story_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Second chain analyzes the story\n",
    "analysis_prompt = PromptTemplate.from_template(\n",
    "    \"Analyze the following story's mood:\\n{story}\"\n",
    ")\n",
    "analysis_chain = analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Combine chains\n",
    "story_with_analysis = story_chain | analysis_chain\n",
    "\n",
    "# Run the combined chain\n",
    "story_analysis = story_with_analysis.invoke({\"topic\": \"a rainy day\"})\n",
    "print(story_analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
